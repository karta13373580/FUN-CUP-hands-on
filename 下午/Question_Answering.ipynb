{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question_Answering.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI5oPesUkjk-",
        "colab_type": "text"
      },
      "source": [
        "# Let's Start Question Answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_Onen8_mGVp",
        "colab_type": "text"
      },
      "source": [
        "使用pytorch-transformers 1.2.0版"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glH-BDVOlW7e",
        "colab_type": "code",
        "outputId": "170f1e87-680b-414e-9d27-8662b49f6b09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "!pip install https://github.com/huggingface/pytorch-transformers/archive/1.2.0.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting https://github.com/huggingface/pytorch-transformers/archive/1.2.0.zip\n",
            "\u001b[?25l  Downloading https://github.com/huggingface/pytorch-transformers/archive/1.2.0.zip (786kB)\n",
            "\u001b[K     |████████████████████████████████| 788kB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.2.0) (1.3.1+cu100)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.2.0) (1.17.4)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.2.0) (1.10.14)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.2.0) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.2.0) (4.28.1)\n",
            "Collecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/8e/cbf2295643d7265e7883326fb4654e643bfc93b3a8a8274d8010a39d8804/regex-2019.11.1-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 3.5MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 42.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 20.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers==1.2.0) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers==1.2.0) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.14 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers==1.2.0) (1.13.14)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.2.0) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.2.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.2.0) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers==1.2.0) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers==1.2.0) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers==1.2.0) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers==1.2.0) (0.14.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.14->boto3->pytorch-transformers==1.2.0) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.14->boto3->pytorch-transformers==1.2.0) (2.6.1)\n",
            "Building wheels for collected packages: pytorch-transformers, sacremoses\n",
            "  Building wheel for pytorch-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-transformers: filename=pytorch_transformers-1.2.0-cp36-none-any.whl size=176431 sha256=0f16d7477cd0f5b5fc04ddcc166c6b0362178ed9e405c461434908e7e10e9ed2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-uy2gvmh1/wheels/45/85/25/47a30e49241bde58fd0a090fe872b7f08b15157cd99e7f456c\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=e3b37f039ebfcba17ae490daaaff9e6ec9da17f1daee1fa287612ba3bd3412ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built pytorch-transformers sacremoses\n",
            "Installing collected packages: regex, sentencepiece, sacremoses, pytorch-transformers\n",
            "Successfully installed pytorch-transformers-1.2.0 regex-2019.11.1 sacremoses-0.0.35 sentencepiece-0.1.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb7dCRxADPSJ",
        "colab_type": "code",
        "outputId": "09701b21-a3ec-4986-8e38-937a5fd19251",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "!rm -rf bert-chinese-qa*\n",
        "!wget -q --no-check-certificate -r 'https://drive.google.com/uc?export=download&id=1GQtGFd-1AvZHZuYckhA3xqvvpDk-x5DW' -O bert-chinese-qa.zip\n",
        "!unzip bert-chinese-qa.zip -d bert-chinese-qa"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  bert-chinese-qa.zip\n",
            "warning [bert-chinese-qa.zip]:  400084 extra bytes at beginning or within zipfile\n",
            "  (attempting to process anyway)\n",
            " extracting: bert-chinese-qa/added_tokens.json  \n",
            "  inflating: bert-chinese-qa/config.json  \n",
            "  inflating: bert-chinese-qa/nbest_predictions_.json  \n",
            "  inflating: bert-chinese-qa/predictions_.json  \n",
            "  inflating: bert-chinese-qa/pytorch_model.bin  \n",
            "  inflating: bert-chinese-qa/special_tokens_map.json  \n",
            "  inflating: bert-chinese-qa/training_args.bin  \n",
            "  inflating: bert-chinese-qa/vocab.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YT_9HJR_8c6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "\n",
        "\n",
        "def to_list(tensor):\n",
        "    return tensor.detach().cpu().tolist()\n",
        " \n",
        "\n",
        "def _get_best_indexes(logits, n_best_size=1):\n",
        "    \"\"\"Get the n-best logits from a list.\"\"\"\n",
        "    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    best_indexes = []\n",
        "    for i in range(len(index_and_score)):\n",
        "        if i >= n_best_size:\n",
        "            break\n",
        "        best_indexes.append(index_and_score[i][0])\n",
        "    return best_indexes\n",
        " \n",
        "\n",
        "def evaluate(dataset, model, tokenizer):\n",
        "    eval_sampler = SequentialSampler(dataset)\n",
        "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=1)\n",
        "\n",
        "    # Eval!\n",
        "    all_results = []\n",
        "    for batch in eval_dataloader:\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        with torch.no_grad():\n",
        "            inputs = {'input_ids':      batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'token_type_ids': batch[2],\n",
        "                      }\n",
        "            example_indices = batch[3]\n",
        "            outputs = model(**inputs)\n",
        "            start_logits = to_list(outputs[0][0])\n",
        "            end_logits   = to_list(outputs[1][0])\n",
        "            start_indexes = _get_best_indexes(start_logits)\n",
        "            end_indexes = _get_best_indexes(end_logits)\n",
        "    return (start_indexes, end_indexes)\n",
        "  \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Dbex7fqbsj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
        "    \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
        "\n",
        "    # Because of the sliding window approach taken to scoring documents, a single\n",
        "    # token can appear in multiple documents. E.g.\n",
        "    #  Doc: the man went to the store and bought a gallon of milk\n",
        "    #  Span A: the man went to the\n",
        "    #  Span B: to the store and bought\n",
        "    #  Span C: and bought a gallon of\n",
        "    #  ...\n",
        "    #\n",
        "    # Now the word 'bought' will have two scores from spans B and C. We only\n",
        "    # want to consider the score with \"maximum context\", which we define as\n",
        "    # the *minimum* of its left and right context (the *sum* of left and\n",
        "    # right context will always be the same, of course).\n",
        "    #\n",
        "    # In the example the maximum context for 'bought' would be span C since\n",
        "    # it has 1 left context and 3 right context, while span B has 4 left context\n",
        "    # and 0 right context.\n",
        "    best_score = None\n",
        "    best_span_index = None\n",
        "    for (span_index, doc_span) in enumerate(doc_spans):\n",
        "        end = doc_span.start + doc_span.length - 1\n",
        "        if position < doc_span.start:\n",
        "            continue\n",
        "        if position > end:\n",
        "            continue\n",
        "        num_left_context = position - doc_span.start\n",
        "        num_right_context = end - position\n",
        "        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
        "        if best_score is None or score > best_score:\n",
        "            best_score = score\n",
        "            best_span_index = span_index\n",
        "\n",
        "    return cur_span_index == best_span_index\n",
        "\n",
        "\n",
        "def convert_examples_to_features(tokenizer, question_text, doc_tokens, max_seq_length=384,\n",
        "                                 doc_stride=128, max_query_length=64,\n",
        "                                 cls_token_at_end=False,\n",
        "                                 cls_token='[CLS]', sep_token='[SEP]', pad_token=0,\n",
        "                                 sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
        "                                 cls_token_segment_id=0, pad_token_segment_id=0,\n",
        "                                 mask_padding_with_zero=True):\n",
        "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "    query_tokens = tokenizer.tokenize(question_text)\n",
        "    if len(query_tokens) > max_query_length:\n",
        "      query_tokens = query_tokens[0:max_query_length]\n",
        "    tok_to_orig_index = []\n",
        "    orig_to_tok_index = []\n",
        "    all_doc_tokens = []\n",
        "    for (i, token) in enumerate(doc_tokens):\n",
        "        orig_to_tok_index.append(len(all_doc_tokens))\n",
        "        sub_tokens = tokenizer.tokenize(token)\n",
        "        for sub_token in sub_tokens:\n",
        "            tok_to_orig_index.append(i)\n",
        "            all_doc_tokens.append(sub_token)\n",
        "\n",
        "    # The -3 accounts for [CLS], [SEP] and [SEP]\n",
        "    max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
        "\n",
        "    # We can have documents that are longer than the maximum sequence length.\n",
        "    # To deal with this we do a sliding window approach, where we take chunks\n",
        "    # of the up to our max length with a stride of `doc_stride`.\n",
        "    _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "        \"DocSpan\", [\"start\", \"length\"])\n",
        "    doc_spans = []\n",
        "    start_offset = 0\n",
        "    while start_offset < len(all_doc_tokens):\n",
        "        length = len(all_doc_tokens) - start_offset\n",
        "        if length > max_tokens_for_doc:\n",
        "            length = max_tokens_for_doc\n",
        "        doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
        "        if start_offset + length == len(all_doc_tokens):\n",
        "            break\n",
        "        start_offset += min(length, doc_stride)\n",
        "\n",
        "    for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
        "        tokens = []\n",
        "        token_to_orig_map = {}\n",
        "        token_is_max_context = {}\n",
        "        segment_ids = []\n",
        "\n",
        "        # p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer)\n",
        "        # Original TF implem also keep the classification token (set to 0) (not sure why...)\n",
        "        p_mask = []\n",
        "\n",
        "        # CLS token at the beginning\n",
        "        if not cls_token_at_end:\n",
        "            tokens.append(cls_token)\n",
        "            segment_ids.append(cls_token_segment_id)\n",
        "            p_mask.append(0)\n",
        "            cls_index = 0\n",
        "\n",
        "        # Query\n",
        "        for token in query_tokens:\n",
        "            tokens.append(token)\n",
        "            segment_ids.append(sequence_a_segment_id)\n",
        "            p_mask.append(1)\n",
        "\n",
        "        # SEP token\n",
        "        tokens.append(sep_token)\n",
        "        segment_ids.append(sequence_a_segment_id)\n",
        "        p_mask.append(1)\n",
        "\n",
        "        # Paragraph\n",
        "        for i in range(doc_span.length):\n",
        "            split_token_index = doc_span.start + i\n",
        "            token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
        "\n",
        "            is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
        "                                                   split_token_index)\n",
        "            token_is_max_context[len(tokens)] = is_max_context\n",
        "            tokens.append(all_doc_tokens[split_token_index])\n",
        "            segment_ids.append(sequence_b_segment_id)\n",
        "            p_mask.append(0)\n",
        "        paragraph_len = doc_span.length\n",
        "\n",
        "        # SEP token\n",
        "        tokens.append(sep_token)\n",
        "        segment_ids.append(sequence_b_segment_id)\n",
        "        p_mask.append(1)\n",
        "\n",
        "        # CLS token at the end\n",
        "        if cls_token_at_end:\n",
        "            tokens.append(cls_token)\n",
        "            segment_ids.append(cls_token_segment_id)\n",
        "            p_mask.append(0)\n",
        "            cls_index = len(tokens) - 1  # Index of classification token\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        while len(input_ids) < max_seq_length:\n",
        "            input_ids.append(pad_token)\n",
        "            input_mask.append(0 if mask_padding_with_zero else 1)\n",
        "            segment_ids.append(pad_token_segment_id)\n",
        "            p_mask.append(1)\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(input_mask) == max_seq_length\n",
        "        assert len(segment_ids) == max_seq_length\n",
        "    input_ids = torch.tensor([input_ids], dtype=torch.long)\n",
        "    input_mask = torch.tensor([input_mask], dtype=torch.long)\n",
        "    segment_ids = torch.tensor([segment_ids], dtype=torch.long)\n",
        "    cls_index = torch.tensor([cls_index], dtype=torch.long)\n",
        "    p_mask = torch.tensor([p_mask], dtype=torch.float)\n",
        "    example_index = torch.arange(input_ids.size(0), dtype=torch.long)\n",
        "    data = TensorDataset(input_ids, input_mask, segment_ids,\n",
        "                            example_index, cls_index, p_mask)\n",
        "\n",
        "\n",
        "    print(\"*** Example ***\")\n",
        "    # print(\"doc_span_index: %s\" % (doc_span_index))\n",
        "    print(\"tokens: %s\" % \" \".join(tokens))\n",
        "    # print(\"token_to_orig_map: %s\" % \" \".join([\n",
        "    #                 \"%d:%d\" % (x, y) for (x, y) in token_to_orig_map.items()]))\n",
        "    # print(\"token_is_max_context: %s\" % \" \".join([\n",
        "    #                 \"%d:%s\" % (x, y) for (x, y) in token_is_max_context.items()\n",
        "    #             ]))\n",
        "    # print(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "    # print(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "    # print(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
        "\n",
        "    return data, tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJqcTOlBmkp5",
        "colab_type": "code",
        "outputId": "5b0a74aa-6fc7-49da-c397-1185110f5248",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 因為要我們今天要跑的是中文QA 所以只有Bert可以用\n",
        "import torch\n",
        "from pytorch_transformers import (WEIGHTS_NAME, BertConfig,\n",
        "                                  BertForQuestionAnswering, BertTokenizer)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "checkpoint = 'bert-chinese-qa'\n",
        "config_class, model_class, tokenizer_class = BertConfig, BertForQuestionAnswering, BertTokenizer\n",
        "model = model_class.from_pretrained(checkpoint).to(device)\n",
        "tokenizer = tokenizer_class.from_pretrained('bert-base-chinese', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 109540/109540 [00:00<00:00, 1872248.07B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6kLJIKjB0jG",
        "colab_type": "code",
        "outputId": "6dc44f06-136f-4b63-db4a-b5dcc04fa886",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "context = '國立臺灣大學，簡稱臺大、NTU，是臺灣第一所現代綜合大學，為臺灣學生人數最多的高等教育機構。其始於1928年日治時代中期創校的「臺北帝國大學」[注 1]，1945年中華民國接收臺灣後經改制與兩次易名始用現名。現設有11個學院、3個專業學院，下分54個學系、109個研究所；另設有30餘個各學術領域之國家級或校級研究中心，以及進修推廣部、臺大醫院等附屬機構，是全臺唯一學生人數超過三萬的高等教育學校[11][14]。目前亦為高教深耕計畫中參與全球鏈結全校型計畫的4所學校之一[15][16]。2020年QS世界大學排名位居第69名。此外，臺大擁有臺北市境內的3大校區、以及多處散布於全臺的分支校區與校地，總面積約3萬4千公頃，佔臺灣土地總面積的百分之一。'\n",
        "question = '國立台灣大學簡稱為什麼'\n",
        "data, tokens = convert_examples_to_features(tokenizer=tokenizer, question_text=question, doc_tokens=context)\n",
        "start, end = evaluate(data, model, tokenizer)\n",
        "\"\".join(tokens[start[0]: end[0]+1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** Example ***\n",
            "tokens: [CLS] 國 立 台 灣 大 學 簡 稱 為 什 麼 [SEP] 國 立 臺 灣 大 學 ， 簡 稱 臺 大 、 n t u ， 是 臺 灣 第 一 所 現 代 綜 合 大 學 ， 為 臺 灣 學 生 人 數 最 多 的 高 等 教 育 機 構 。 其 始 於 1 9 2 8 年 日 治 時 代 中 期 創 校 的 「 臺 北 帝 國 大 學 」 [ 注 [SEP] 1 ] ， 1 9 4 5 年 中 華 民 國 接 收 臺 灣 後 經 改 制 與 兩 次 易 名 始 用 現 名 。 現 設 有 1 1 個 學 院 、 3 個 專 業 學 院 ， 下 分 5 4 個 學 系 、 1 0 9 個 研 究 所 ； 另 設 有 3 0 餘 個 各 學 術 領 域 之 國 家 級 或 校 級 研 究 中 心 ， 以 及 進 修 推 廣 部 、 臺 大 醫 院 等 附 屬 機 構 ， 是 全 臺 唯 一 學 生 人 數 超 過 三 萬 的 高 等 教 育 學 校 [ 1 1 ] [ 1 4 ] 。 目 前 亦 為 高 教 深 耕 計 畫 中 參 與 全 球 鏈 結 全 校 型 計 畫 的 4 所 學 校 之 一 [ 1 5 ] [ 1 6 ] 。 2 0 2 0 年 q s 世 界 大 學 排 名 位 居 第 6 9 名 。 此 外 ， 臺 大 擁 有 臺 北 市 境 內 的 3 大 校 區 、 以 及 多 處 散 布 於 全 臺 的 分 支 校 區 與 校 地 ， 總 面 積 約 3 萬 4 千 公 頃 ， 佔 臺 灣 土 地 總 面 積 的 百 分 之 一 。 [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'臺大'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIjG57MsT2BC",
        "colab_type": "code",
        "outputId": "d12f5ff6-78a0-4dd1-9546-807bb5f2e80f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "question = '台大面積有多大？'\n",
        "data, tokens = convert_examples_to_features(tokenizer=tokenizer, question_text=question, doc_tokens=context)\n",
        "start, end = evaluate(data, model, tokenizer)\n",
        "\"\".join(tokens[start[0]: end[0]+1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** Example ***\n",
            "tokens: [CLS] 台 大 面 積 有 多 大 ？ [SEP] 國 立 臺 灣 大 學 ， 簡 稱 臺 大 、 n t u ， 是 臺 灣 第 一 所 現 代 綜 合 大 學 ， 為 臺 灣 學 生 人 數 最 多 的 高 等 教 育 機 構 。 其 始 於 1 9 2 8 年 日 治 時 代 中 期 創 校 的 「 臺 北 帝 國 大 學 」 [ 注 [SEP] 1 ] ， 1 9 4 5 年 中 華 民 國 接 收 臺 灣 後 經 改 制 與 兩 次 易 名 始 用 現 名 。 現 設 有 1 1 個 學 院 、 3 個 專 業 學 院 ， 下 分 5 4 個 學 系 、 1 0 9 個 研 究 所 ； 另 設 有 3 0 餘 個 各 學 術 領 域 之 國 家 級 或 校 級 研 究 中 心 ， 以 及 進 修 推 廣 部 、 臺 大 醫 院 等 附 屬 機 構 ， 是 全 臺 唯 一 學 生 人 數 超 過 三 萬 的 高 等 教 育 學 校 [ 1 1 ] [ 1 4 ] 。 目 前 亦 為 高 教 深 耕 計 畫 中 參 與 全 球 鏈 結 全 校 型 計 畫 的 4 所 學 校 之 一 [ 1 5 ] [ 1 6 ] 。 2 0 2 0 年 q s 世 界 大 學 排 名 位 居 第 6 9 名 。 此 外 ， 臺 大 擁 有 臺 北 市 境 內 的 3 大 校 區 、 以 及 多 處 散 布 於 全 臺 的 分 支 校 區 與 校 地 ， 總 面 積 約 3 萬 4 千 公 頃 ， 佔 臺 灣 土 地 總 面 積 的 百 分 之 一 。 [SEP]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3萬4千公頃'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}